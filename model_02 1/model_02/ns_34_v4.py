# -*- coding: utf-8 -*-
"""ns_34_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1baLw1kNrAv6U1C3V4vLjrC4xBQ1BKcGM
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np

###################################### Some helper functions for loading and saving trained models
def load_model(path, model, optimizer, scheduler=None):
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    if scheduler is None: return model, optimizer
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    return model, optimizer, scheduler

def save_model(path, model, optimizer, scheduler=None):
    data = {'model_state_dict': model.state_dict()}
    if optimizer is not None: data['optimizer_state_dict'] = optimizer.state_dict()
    if scheduler is not None: data['scheduler_state_dict'] = scheduler.state_dict()
    torch.save(data, path)

###################################### Some helper functions for retrieving CSV data (mesh and cfd results)
def parse_csv_data(path, category, batch_size, device):
    if category == 'mesh':
        x, y, z = np.loadtxt(path, delimiter=',', unpack=True)
        x = torch.tensor(x, dtype=torch.float, requires_grad=True, device=device).reshape(-1,1)
        y = torch.tensor(y, dtype=torch.float, requires_grad=True, device=device).reshape(-1,1)
        z = torch.tensor(z, dtype=torch.float, requires_grad=True, device=device).reshape(-1,1)
        return DataLoader(TensorDataset(x, y, z), batch_size=batch_size, shuffle=True)
    elif category == 'cfd':
        x, y, z, p, u, v, w = np.loadtxt(path, delimiter=',', unpack=True)
        x = torch.tensor(x, dtype=torch.float, device=device).reshape(-1,1)
        y = torch.tensor(y, dtype=torch.float, device=device).reshape(-1,1)
        z = torch.tensor(z, dtype=torch.float, device=device).reshape(-1,1)
        u = torch.tensor(u, dtype=torch.float, device=device).reshape(-1,1)
        v = torch.tensor(v, dtype=torch.float, device=device).reshape(-1,1)
        w = torch.tensor(w, dtype=torch.float, device=device).reshape(-1,1)
        p = torch.tensor(p, dtype=torch.float, device=device).reshape(-1,1)
        return DataLoader(TensorDataset(x, y, z, p, u, v, w), batch_size=batch_size, shuffle=True)

###################################### The neural network. \R^3 to \R^4
class NSNeuralNet(nn.Module):
    def __init__(self, width=256):
        super().__init__()
        self.main = nn.Sequential(
            nn.Linear(3,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,width),
            nn.SiLU(),
            nn.Linear(width,4)
        )

    def forward(self, x, y, z):
        return self.main(torch.cat([x, y, z], axis=1))

###################################### Define the loss functions
loss_func = nn.MSELoss()

def loss_physics(net, x, y, z):
    qq = net(x, y, z)
    p = qq[:,0].reshape(-1,1)
    u = qq[:,1].reshape(-1,1)
    v = qq[:,2].reshape(-1,1)
    w = qq[:,3].reshape(-1,1)
    del qq

    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]
    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(y), create_graph=True)[0]
    p_z = torch.autograd.grad(p, z, grad_outputs=torch.ones_like(z), create_graph=True)[0]

    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]
    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]
    w_x = torch.autograd.grad(w, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]

    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(y), create_graph=True)[0]
    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(y), create_graph=True)[0]
    w_y = torch.autograd.grad(w, y, grad_outputs=torch.ones_like(y), create_graph=True)[0]

    u_z = torch.autograd.grad(u, z, grad_outputs=torch.ones_like(z), create_graph=True)[0]
    v_z = torch.autograd.grad(v, z, grad_outputs=torch.ones_like(z), create_graph=True)[0]
    w_z = torch.autograd.grad(w, z, grad_outputs=torch.ones_like(z), create_graph=True)[0]

    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]
    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]
    w_xx = torch.autograd.grad(w_x, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]

    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(y), create_graph=True)[0]
    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(y), create_graph=True)[0]
    w_yy = torch.autograd.grad(w_y, y, grad_outputs=torch.ones_like(y), create_graph=True)[0]

    u_zz = torch.autograd.grad(u_z, z, grad_outputs=torch.ones_like(z), create_graph=True)[0]
    v_zz = torch.autograd.grad(v_z, z, grad_outputs=torch.ones_like(z), create_graph=True)[0]
    w_zz = torch.autograd.grad(w_z, z, grad_outputs=torch.ones_like(z), create_graph=True)[0]

    rho = 1050 # density
    mu = 0.0035 # viscosity

    eqn_x = p_x + rho * (u*u_x + v*u_y + w*u_z) - mu * (u_xx + u_yy + u_zz)
    eqn_y = p_y + rho * (u*v_x + v*v_y + w*v_z) - mu * (v_xx + v_yy + v_zz)
    eqn_z = p_z + rho * (u*w_x + v*w_y + w*w_z) - mu * (w_xx + w_yy + w_zz)
    eqn_c = u_x + v_y + w_z
    dt = torch.cat([eqn_x, eqn_y, eqn_z, eqn_c], axis=1)
    return loss_func(dt, torch.zeros_like(dt))

def loss_data(net, x, y, z, p, u, v, w):
    return loss_func(net(x, y, z), torch.cat([p, u, v, w], axis=1))

###################################### SETUP THE NEURAL NETWORK
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = NSNeuralNet().to(device)
model.apply(lambda m: nn.init.kaiming_normal_(m.weight) if type(m) == nn.Linear else None);

###################################### Training parameters
epochs = 50

load_previous = False
save_end = True
load_fn = 'ns_34_3'
save_fn = 'ns_34_3'

print_every = 5
save_every = 100

learning_rate = 1e-3
lr_steps = 100
lr_gamma = 0.5
lambda_data = 10.0

file_mesh = 'mesh_1_s.csv'
file_cfd = 'train_1.csv'
batch_mesh = 5000 #5000
batch_cfd = 2500 #2500

###################################### SETUP THE NEURAL NETWORK
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_steps, gamma=lr_gamma)
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')

###################################### LOAD A PRETRAINED MODEL

if load_previous: model, optimizer, scheduler = load_model(f'{load_fn}.pt', model, optimizer, scheduler)

###################################### TRAIN THE LOADED MODEL

print(f'Starting the training phase on {device} ...')

mesh_ds = parse_csv_data(file_mesh, 'mesh', batch_mesh, device)
cfd_ds = parse_csv_data(file_cfd, 'cfd', batch_cfd, device)

for epoch in range(1,epochs+1):
    for xm, ym, zm in mesh_ds:
        for xd, yd, zd, pd, ud, vd, wd in cfd_ds:
            l_phys = loss_physics(model, xm, ym, zm)
            l_data = loss_data(model, xd, yd, zd, pd, ud, vd, wd)
            l_total = l_phys + lambda_data * l_data

            optimizer.zero_grad()

            l_total.backward()

            optimizer.step()

    scheduler.step()

    if epoch % 5 == 0:
        with torch.autograd.no_grad():
            msg = f'[{epoch:>5} / {epochs:<5}]'
            msg += f'    Ph( {l_phys:>11.7f} )'
            msg += f'    Dt( {l_data:>11.7f} )'
            msg += f'    Tt( {l_total:>11.7f} )'
            print(msg)

    if epoch % save_every == 0: save_model(f'{save_fn}_{epoch}.pt', model, optimizer, scheduler)

print('Training phase complete.')

###################################### SAVE THE TRAINED MODEL

if save_end: save_model(f'{save_fn}.pt', model, optimizer, scheduler)

